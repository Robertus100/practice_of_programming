% vim: ts=4 sts=4 sw=4 et tw=75
\chapter{Portability}
\label{chap:portability}
\begin{quote}
    Finally, standardization, like convention, can be another manifestation
    (证明) of the strong older. But unlike convention it has been accepted
    in Modern architecture as an enriching (丰富) product of our
    technology, yet dreaded (可怕的) for its potential domination and
    brutality.
\end{quote}
\begin{quotesrc}
    Robert Venturi, \bookname{Complexity and Contradiction (矛盾) in
Architecture}
\end{quotesrc}

It's hard to write software that runs correctly and efficiently. So once a
program works in one environment, you don't want to repeat much of the
effort if you move it to a different compiler or processor or operating
system. Ideally (理想地), it should need no changes whatsoever (无论什么).

This ideal is called \emph{portability}. In practice, "portability" more
often stands for the weaker concept that it will be easier to modify the
program as it moves than to rewrite it from scratch (擦除). The less
revision it needs, the more portable it is.

You may wonder why we worry about portability at all. If software is going
to run in only one environment, under specified conditions, why spend time
giving it broader applicability? First, any successful program, almost by
definition, gets used in unexpected ways and unexpected places. Building
software to be more general than its original specification will result in
less maintenance and more utility down the road. Second, environments
change. When the compiler or operating system or hardware is upgraded,
features may change. The less the program depends on special features, the
less likely it is to break and the more easily it will adapt to changing
circumstances. Finally, and most important, a portable program is a better
program. The effort invested to make a program portable also makes it
better designed, better constructed, and more thoroughly tested. The
techniques of portable programming are closely related to the techniques of
good programming in general.

Of course the degree of portability must be tempered (调和) by reality.
There is no such thing as an absolutely portable program, only a program
that hasn't yet been tried in enough environments. But we can keep
portability as our goal by aiming towards software that runs without change
almost everywhere. Even if this goal isn't met completely, time spent on
portability as the program is created will pay off when the software must
be updated.

Our message is this: try to write software that works within the
intersection of the various standards, interfaces and environments it must
accommodate. Don't fix every portability problem by adding special code;
instead, adapt the software to work within the new constraints. Use
abstraction and encapsulation to restrict and control unavoidable
non-portable code. By staying within the intersection of constraints and by
localizing (局部化) system dependencies, your code will become cleaner and
more general as it is ported (移值).

\section{Language}
\label{sec:language}

\emph{Stick to the standard.} The first step to portable code is of course
to program in a high-level language, and within the language standard if
there is one.  Binaries don't port well, but source code does. Even so, the
way that a compiler translates a program into machine instructions is not
precisely defined, even for standard languages.  Few languages in wide use
have only a single implementation; there are usually multiple suppliers,
or versions for different operating systems, or releases that have evolved
over time. How they interpret your source code will vary.

Why isn't a standard a strict definition? Sometimes a standard is
incomplete and fails to define the behavior when features interact
(互相干扰). Sometimes it's deliberately (故意地) indefinite (不确定); for
example, the \verb'char' type in C and C++ may be signed or unsigned, and
need not even have exactly 8 bits. Leaving such issues up to the compiler
writer may allow more efficient implementations and avoid restricting the
hardware the language will run on, at the risk of making life harder for
programmers. Politics and technical compatibility issues may lead to
compromises that leave details unspecified.  Finally, languages are
intricate (复杂的) and compilers are complex; there will be errors in the
interpretation and bugs in the implementation.

Sometimes the languages aren't standardized at all. C has an official
ANSI/ISO standard issued in 1988, but the ISO C++ standard was ratified
(批准) only in 1998; at the time we are writing this, not all compilers in
use support the official definition. Java is new and still years away from
standardization. A language standard is usually developed only after the
language has a variety of conflicting (互相矛盾的) implementations to unify
(统一), and is in wide enough use to justify the expense of
standardization.  In the meantime, there are still programs to write and
multiple environments to support.

So although reference manuals and standards give the impression (印象) of
rigorous (严格的) specification, they never define a language fully, and
different implementations may make valid but incompatible interpretations.
Sometimes there are even errors. A small illustration showed up while we
were first writing this chapter. This external declaration is illegal in C
and C++:
\begin{badcode}
    *x[] = {"abc"};
\end{badcode}
A test of a dozen compilers turned up a few that correctly diagnosed the
missing \verb'char' type specifier for \verb'x', a fair number that warned
of mismatched types (apparently using an old definition of the language to
infer (推断) incorrectly that \verb'x' is an array of \verb'int' pointers),
and a couple that compiled the illegal code without a murmur (低语) of
complaint.

\emph{Program in the mainstream.} The inability of some compilers to flag
this error is unfortunate, but it also indicates an important aspect of
portability.  Languages have dark comers (死角) where practice varies --
bit-fields in C and C++, for example -- and it is prudent (谨慎的) to avoid
them. Use only those features for which the language definition is
unambiguous and well understood. Such features are more likely to be widely
available and to behave the same way everywhere. We call this the
mainstream of the language.

It's hard to know just where the mainstream is, but it's easy to recognize
constructions that are well outside it. Brand new features such as
\verb'//' comments and \verb'complex' in C, or features specific to one
architecture such as the keywords \verb'near' and \verb'far', are
guaranteed to cause trouble. If a feature is so unusual or unclear that to
understand it you need to consult a "language lawyer" -- an expert in
reading language definitions -- don't use it.

In this discussion, we'll focus on C and C++, general-purpose languages
commonly used to write portable software. The C standard is more than a
decade old and the language is very stable, but a new standard is in the
works, so upheaval (变动) is coming.  Meanwhile, the C++ standard is hot
off the press (刚刚出炉), so not all implementations have had time to
converge (聚合).

What is the C mainstream? The term usually refers to the established style
of use of the language, but sometimes it's better to plan for the future.
For example, the original version of C did not require function prototypes.
One declared \verb'sqrt' to be a function by saying
\begin{badcode}
    double sqrt();
\end{badcode}
which defines the type of the return value but not of the parameters. ANSI
C added function prototypes, which specify everything:
\begin{wellcode}
    double sqrt(double);
\end{wellcode}
ANSI C compilers are required to accept the earlier syntax, but you should
nonetheless (仍然) write prototypes for all your functions. Doing so will
guarantee safer code -- function calls will be fully type-checked -- and if
interfaces change, the compiler will catch them. If your code calls
\begin{wellcode}
    func(7, PI);
\end{wellcode}
but func has no prototype, the compiler might not verify that \verb'func'
is being called correctly. If the library later changes so that \verb'func'
has three arguments, the need to repair the software might be missed
because the old-style syntax disables type checking of function arguments.

C++ is a larger language with a more recent standard, so its mainstream is
harder to identify. For example, although we expect the STL to become
mainstream, this will not happen immediately, and some current
implementations do not support it completely.

\emph{Beware of language trouble spots (故障点).} As we mentioned, standard
leave some things intentionally undefined or unspecified, usually to give
compiler writers more flexibility. The list of such behaviors is
discouragingly long.

\textbf{Size of data types.} The sizes of basic data types in C and C++ are
not defined; other than (除了) the basic rules that
\begin{wellcode}
    sizeof(char) <= sizeof(short) <= sizeof(int) <= sizeof(long)
    sizeof(float) <= sizeof(double)
\end{wellcode}
and that \verb'char' must have at least 8 bit, \verb'short' and \verb'int'
at least 16, and \verb'long' at least 32, there are no guaranteed
properties. It's not even required that a pointer value fit in an
\verb'int'.

It's easy enough to find out what the sizes are for a specific compiler:
\begin{wellcode}
    /* sizeof: display sizes of basic types */
    int main(void)
    {
        printf("char %d, short %d, int %d, long %d",
                sizeof(char), sizeof(short),
                sizeof(int), sizeof(long));
        printf(" float %d, double %d, void* %d\n",
                sizeof(float), sizeof(double), sizeof(void *));
        return 0;
    }
\end{wellcode}
The output is the same on most of the machines we use regularly:
\begin{wellcode}
    char 1, short 2, int 4, long 4, float 4, double 8, void* 4
\end{wellcode}
but other values are certainly possible. Some 64-bit machines produce this:
\begin{wellcode}
    char 1, short 2, int 4, long 8, float 4, double 8, void* 8
\end{wellcode}
and early PC compilers typically produced this:
\begin{wellcode}
    char 1, short 2, int 2, long 4, float 4, double 8, void* 2
\end{wellcode}
In the early days of PCs, the hardware supported several kinds of pointers.
Coping with this mess (混乱) caused the invention (发明) of pointer
modifiers like \verb'far' and \verb'near', neither of which is standard,
but whose reserved-word ghosts (灵魂) still haunt (出没于) current
compilers. If your compiler can change the sizes of basic types, or if you
have machines with different sizes, try to compile and test your program in
these different configurations.

The standard header file \verb'stddef.h' defines a number of types that can
help with portability. The most commonly-used of these is \verb'size_t',
which is the unsigned integer type returned by the \verb'sizeof' operator.
Values of this type are returned by functions like \verb'strlen' and used
as arguments by many functions, including \verb'malloc'.

Learning from some of these experiences, Java defines the sizes of all
basic data types: \verb'byte' is 8 bits, \verb'char' and \verb'short' are
16, \verb'int' is 32, and \verb'long' is 64.

We will ignore the rich set of potential issues related to floating-point
computation since that is a book-sized topic in itself. Fortunately, most
modern machines support the IEEE standard for floating-point hardware, and
thus the properties of floating-point arithmetic are reasonably well
defined.

\textbf{Order of evaluation.} In C and C++, the order of evaluation of
operands of expressions, side effects, and function arguments is not
defined. For example, in the assignment
\begin{badcode}
    n = (getchar() << 8) | getchar();
\end{badcode}
the second \verb'getchar' could be called first: the way the expression is
written is not necessarily the way it executes. In the statement
\begin{badcode}
    ptr[count] = name[++count];
\end{badcode}
\verb'count' might be incremented before or after it is used to index
\verb'ptr', and in
\begin{badcode}
    printf("%c %c\n", getchar(), getchar());
\end{badcode}
the first input character could be printed second instead of first. In
\begin{badcode}
    printf("%f %s\n", log(-1.23), strerror(errno));
\end{badcode}
the value of \verb'errno' may be evaluated before \verb'log' is called.

There are rules for when certain expressions are evaluated. By definition,
all side effects and function calls must be completed at each semicolon, or
when a function is called. The \verb'&&' and \verb'||' operators execute
left to right and only as far as necessary to determine their truth value
(including side effects). The condition in a \verb'?:' operator is
evaluated (including side effects) and then exactly one of the two
expressions that follow is evaluated.

Java has a stricter definition of order of evaluation. It requires that
expressions, including side effects, be evaluated left to right, though
(虽然) one authoritative (权威的) manual advises not writing code that
depends "crucially (重要的)" on this behavior. This is sound (合理的)
advice if there's any chance that Java code will be converted to C or C++,
which make no such promises (约定). Converting between languages is an
extreme but occasionally reasonable test of portability.

\textbf{Signedness of \texttt{char}.} In C and C++, it is not specified
whether the \verb'char' data type is signed or unsigned. This can lead to
trouble when combining \verb'char's and \verb'int's, such as in code that
calls the \verb'int'-valued routine \verb'getchar()'. If you say
\begin{wellcode}
    char    c;  /* should be int */
    c = getchar();
\end{wellcode}
the value of \verb'c' will be between 0 and 255 if \verb'char' is unsigned,
and between -128 and 127 if \verb'char' is signed, for the almost universal
configuration of 8-bit characters on a two's complement machine. This has
implications if the character is to be used as an array subscript or if it
is to be tested against \verb'EOF', which usually has value -1 in
\verb'stdio'.  For instance, we had developed this code in Section
\ref{sec:test_as_you_write_code} after fixing a few boundary conditions in
the original version. The comparison \verb's[i] == EOF' will always fail if
\verb'char' is unsigned:
\begin{badcode}
    int i;
    char s[MAX];

    for (i = 0; i < MAX - 1; i++)
        if ((s[i] = getchar()) == '\n' || s[i] == EOF)
            break;
    s[i] = '\0';
\end{badcode}
When \verb'getchar' returns \verb'EOF', the value \verb'255' (\verb'0xff',
the result of converting \verb'-1' to \verb'unsigned char') will be stored
in \verb's[i]', If \verb's[i]' is unsigned, this will remain 255 for the
comparison with \verb'EOF', which will fail.

Even if \verb'char' is signed, however, the code isn't correct. The
comparison will succeed at \verb'EOF', but a valid input byte of
\verb'0xff' will look just like \verb'EOF' ad terminate the loop
prematurely (过早). So regardless of the sign of \verb'char', you must
always store the return value of \verb'getchar' in an \verb'int' for
comparison with \verb'EOF'. Here is how to write the loop portably:
\begin{wellcode}
    int c, i;
    char s[MAX];

    for (i = 0; i < MAX - 1; i++) {
        if ((c = getchar()) == '\n' || c == EOF)
            break;
        s[i] = c;
    }
    s[i] = '\0';
\end{wellcode}

Java has no \verb'unsigned' qualifier; integral types are signed and the
(16-bit) \verb'char' type is not.

\textbf{Arithmetic or logical shift.} Right shifts of signed quantities
with \verb'>>' operator may be arithmetic (a copy of the sign bit is
propagated (可繁殖的) during the shift) or logical (zeros fill the vacated
(空出的) bits during the shift). Again, learning from the problems with C
and C++, Java reserves \verb'>>' for arithmetic right shift and provides a
separate operator \verb'>>>' for logical right shift.

\textbf{Byte order.} The byte order within \verb'short', \verb'int', and
\verb'long' is not defined; the byte with the lowest address may be the
most significant byte or the least significant byte. This is a
hardware-dependent issue that we'll discuss later in this chapter.

\textbf{Alignment of structure and class members.} The alignment of items
within structures, classes, and unions is not defined, except that members
are laid out in the order of declaration. For example, in this structure
\begin{wellcode}
    struct X {
        char    c;
        int     i;
    };
\end{wellcode}
the address of \verb'i' could be 2,4, or 8 bytes from the beginning of the
structure. A few machines allow \verb'int's to be stored on odd boundaries,
but most demand that an n-byte primitive data type be stored at an n-byte
boundary, for example that \verb'doubles', which are usually 8 bytes long,
are stored at addresses that are multiples of 8.  On top of this, the
compiler writer may make further (更多的) adjustments, such as forcing
alignment for performance reasons.

You should never assume that the elements of a structure occupy contiguous
memory. Alignment restrictions introduce "holes"; \verb'struct X' will have
at least one byte of unused space. These holes imply that a structure may
be bigger than the sum of its member sizes, and will vary from machine to
machine. If you're allocating memory to hold one, you must ask for
\texttt{sizeof (struct X)} bytes, not \verb'sizeof(char) + sizeof(int)'.

\textbf{Bitfields.} Bitfields are so machine-dependent that no one should
use them.

This long list of perils (危险) can be skirted (绕过) by following a few
rules. Don't use side effects except for a very few idiomatic constructions
like
\begin{wellcode}
    a[i++] = 0;
    c = *p++;
    *s++ = *t++;
\end{wellcode}
Don't compare a \verb'char' to \verb'EOF'. Always use \verb'sizeof' to
compute the size of types and objects. Never right shift a signed value.
Make sure the data type is big enough for the range of values you are
storing in it.

\emph{Try several compilers.} It's easy to think that you understand
portability, but compilers will see problems that you don't, and different
compilers sometimes see your program differently, so you should take
advantage of their help. Turn on all compiler warnings. Try multiple
compilers on the same machine and on different machines.  Try a C++
compiler on a C program.

Since the language accepted by different compilers varies, the fact that
your program compiles with one compiler is no guarantee that it is even
syntactically correct.  If several compilers accept your code, however, the
odds (奇怪的事情) improve. We have compiled every C program in this book
with three C compilers on three unrelated operating systems (Unix, Plan 9,
Windows) and also a couple of C++ compilers. This was a sobering (合理的)
experience, but it caught dozens of portability errors that no amount of
human scrutiny (检查) would have uncovered. They were all trivial (琐细的)
to fix.

Of course, compilers cause portability problems too, by making different
choices for unspecified behaviors. But our approach still gives us hope.
Rather than writing code in a way that amplifies (放大) the differences
among systems, environments, and compilers, we strive (争取) to create
software that behaves independently of the variations. In short, we steer
clear of (避开) features and properties that are likely to vary.

\section{Headers and Libraries}
\label{sec:header_library}

Headers and libraries provide services that augment (增加) the basic
language.  Examples include input and output through \verb'stdio' in C,
\verb'iostream' in C++, and \verb'java.io' in Java.  Strictly speaking,
these are not part of the language, but they are defined along with the
language itself and are expected to be part of any environment that claims
to support it. But because libraries cover a broad spectrum (范围) of
activities, and must often deal with operating system issues, they can
still harbor (港湾) non-portabilities.

\emph{Use standard libraries.} The same general advice applies here as for
the core language: stick to the standard, and within its older,
well-established components. C defines a standard library of functions for
input and output, string operations, character class tests, storage
allocation, and a variety of other tasks. If you confine (禁止) your
operating system interactions to these functions, there is a good chance
that your code will behave the same way and perform well as it moves from
system to system. But you must still be careful, because there are many
implementations of the library and some of them contain features that are
not defined in the standard.

ANSI C does not define the string-copying function \verb'strdup', yet most
environments provide it, even those that claim to conform (一致) to the
standard. A seasoned (老练的) programmer may use \verb'strdup' out of habit
(出于习惯), and not be warned that it is non-standard.  Later, the program
will fail to compile when ported to an environment that does not provide
the function. This sort of problem is the major portability headache
introduced by libraries; the only solution is to stick to the standard and
test your program in a wide variety of environments.

Header files and package definitions declare the interface to standard
functions.  One problem is that headers tend to be cluttered (杂乱的)
because they are trying to cope (应付) with several languages in the same
file. For example, it is common to find a single header file like
\verb'stdio.h' serving pre-ANSI C, ANSI C, and even C++ compilers. In such
cases, the file is littered (弄乱) with conditional compilation directives
like \verb'#if' and \verb'#ifdef'.  Because the preprocessor language is
not very flexible, the files are complicated and hard to read, and
sometimes contain errors.

This excerpt (摘录) from a header file on one of our systems is better than
most, because it is neatly formatted:
\begin{wellcode}
    #ifdef __OLD_C
        extern int fread();
        extern int fwrite();
    #else
        #if defined(__STDC__) || defined(__cplusplus)
            extern size_t fread(void *, size_t, size_t, FILE *);
            extern size_t fwrite(const void *, size_t, size_t, FILE *);
        #else /* not __STDC__ __cplusplus */
            extern size_t fread();
            extern size_t fwrite();
        #endif /* else not __STDC__ || __cplusplus */
    #endif
\end{wellcode}
Even though the example is relatively clean, it demonstrates that header
files (and programs) structured like this are intricate (复杂的) and hard
to maintain. It might be easier to use a different header for each compiler
or environment. This would require maintaining separate files, but each
would be self-contained and appropriate for a particular system, and would
reduce the likelihood of errors like including \verb'strdup' in a strict
ANSI C environment.

Header files also can "pollute" the name space by declaring a function with
the same name as one in your program. For example, our warning-message
function \verb'weprintf' was originally called \verb'wprintf', but we
discovered that some environments, in anticipation (预测) of the new C
standard, define a function with that name in \verb'stdio.h'.  We needed to
change the name of our function in order to compile on those systems and be
ready for the future. If the problem was an erroneous implementation rather
than a legitimate (合法的) change of specification, we could work around it
by redefining the name when including the header:
\begin{badcode}
    /* some versions of stdio use wprintf so define it away */
    #define wprintf stdio_wprintf
    #undef wprintf
    /* code using our wprintf() follows ... */
\end{badcode}
This maps all occurrences of \verb'wprintf' in the header file to
\verb'stdio_wprintf' so they will not interfere with our version. We can
then use our own \verb'wprintf' without changing its name, at the cost of
some clumsiness (笨拙) and the risk that a library we link with will call
our \verb'wprintf' expecting to get the official one. For a single
function, it's probably not worth the trouble, but some systems make such a
mess of the environment that one must resort to (采取) extremes (极端方法)
to keep the code clean. Be sure to comment what the construction is doing,
and don't make it worse by adding conditional compilation. If some
environments define \verb'wprintf', assume they all do; then the fix is
permanent and you won't have to maintain the \verb'#ifdef' statements as
well.  It may be easier to switch than fight and it's certainly safer, so
that's what we did when we changed the name to \verb'weprintf'.

Even if you try to stick to the rules and the environment is clean, it is
easy to step outside the limits by implicitly assuming that some favorite
property is true everywhere. For instance, ANSI C defines six signals that
can be caught with \verb'signal'; the POSIX standard defines 19; most Unix
systems support 32 or more. If you want to use a non-ANSI signal, there is
clearly a tradeoff (交易) between functionality and portability, and you
must decide which matters more.

There are many other standards that are not part of a programming language
definition; examples include operating system and network interfaces,
graphics interfaces, and the like. Some are meant to carry across more than
one system, like POSIX; others are specific to one system, like the various
Microsoft Windows APIs.  Similar advice holds here as well. Your programs
will be more portable if you choose widely used and well-established
standards, and if you stick to the most central and commonly used aspects.
