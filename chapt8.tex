% vim: ts=4 sts=4 sw=4 et tw=75
\chapter{Portability}
\label{chap:portability}
\begin{quote}
    Finally, standardization, like convention, can be another manifestation
    (证明) of the strong older. But unlike convention it has been accepted
    in Modern architecture as an enriching (丰富) product of our
    technology, yet dreaded (可怕的) for its potential domination and
    brutality.
\end{quote}
\begin{quotesrc}
    Robert Venturi, \bookname{Complexity and Contradiction (矛盾) in
Architecture}
\end{quotesrc}

It's hard to write software that runs correctly and efficiently. So once a
program works in one environment, you don't want to repeat much of the
effort if you move it to a different compiler or processor or operating
system. Ideally (理想地), it should need no changes whatsoever (无论什么).

This ideal is called \emph{portability}. In practice, "portability" more
often stands for the weaker concept that it will be easier to modify the
program as it moves than to rewrite it from scratch (擦除). The less
revision it needs, the more portable it is.

You may wonder why we worry about portability at all. If software is going
to run in only one environment, under specified conditions, why spend time
giving it broader applicability? First, any successful program, almost by
definition, gets used in unexpected ways and unexpected places. Building
software to be more general than its original specification will result in
less maintenance and more utility down the road. Second, environments
change. When the compiler or operating system or hardware is upgraded,
features may change. The less the program depends on special features, the
less likely it is to break and the more easily it will adapt to changing
circumstances. Finally, and most important, a portable program is a better
program. The effort invested to make a program portable also makes it
better designed, better constructed, and more thoroughly tested. The
techniques of portable programming are closely related to the techniques of
good programming in general.

Of course the degree of portability must be tempered (调和) by reality.
There is no such thing as an absolutely portable program, only a program
that hasn't yet been tried in enough environments. But we can keep
portability as our goal by aiming towards software that runs without change
almost everywhere. Even if this goal isn't met completely, time spent on
portability as the program is created will pay off when the software must
be updated.

Our message is this: try to write software that works within the
intersection of the various standards, interfaces and environments it must
accommodate. Don't fix every portability problem by adding special code;
instead, adapt the software to work within the new constraints. Use
abstraction and encapsulation to restrict and control unavoidable
non-portable code. By staying within the intersection of constraints and by
localizing (局部化) system dependencies, your code will become cleaner and
more general as it is ported (移值).

\section{Language}
\label{sec:language}

\emph{Stick to the standard.} The first step to portable code is of course
to program in a high-level language, and within the language standard if
there is one.  Binaries don't port well, but source code does. Even so, the
way that a compiler translates a program into machine instructions is not
precisely defined, even for standard languages.  Few languages in wide use
have only a single implementation; there are usually multiple suppliers,
or versions for different operating systems, or releases that have evolved
over time. How they interpret your source code will vary.

Why isn't a standard a strict definition? Sometimes a standard is
incomplete and fails to define the behavior when features interact
(互相干扰). Sometimes it's deliberately (故意地) indefinite (不确定); for
example, the \verb'char' type in C and C++ may be signed or unsigned, and
need not even have exactly 8 bits. Leaving such issues up to the compiler
writer may allow more efficient implementations and avoid restricting the
hardware the language will run on, at the risk of making life harder for
programmers. Politics and technical compatibility issues may lead to
compromises that leave details unspecified.  Finally, languages are
intricate (复杂的) and compilers are complex; there will be errors in the
interpretation and bugs in the implementation.

Sometimes the languages aren't standardized at all. C has an official
ANSI/ISO standard issued in 1988, but the ISO C++ standard was ratified
(批准) only in 1998; at the time we are writing this, not all compilers in
use support the official definition. Java is new and still years away from
standardization. A language standard is usually developed only after the
language has a variety of conflicting (互相矛盾的) implementations to unify
(统一), and is in wide enough use to justify the expense of
standardization.  In the meantime, there are still programs to write and
multiple environments to support.

So although reference manuals and standards give the impression (印象) of
rigorous (严格的) specification, they never define a language fully, and
different implementations may make valid but incompatible interpretations.
Sometimes there are even errors. A small illustration showed up while we
were first writing this chapter. This external declaration is illegal in C
and C++:
\begin{badcode}
    *x[] = {"abc"};
\end{badcode}
A test of a dozen compilers turned up a few that correctly diagnosed the
missing \verb'char' type specifier for \verb'x', a fair number that warned
of mismatched types (apparently using an old definition of the language to
infer (推断) incorrectly that \verb'x' is an array of \verb'int' pointers),
and a couple that compiled the illegal code without a murmur (低语) of
complaint.

\emph{Program in the mainstream.} The inability of some compilers to flag
this error is unfortunate, but it also indicates an important aspect of
portability.  Languages have dark comers (死角) where practice varies --
bit-fields in C and C++, for example -- and it is prudent (谨慎的) to avoid
them. Use only those features for which the language definition is
unambiguous and well understood. Such features are more likely to be widely
available and to behave the same way everywhere. We call this the
mainstream of the language.

It's hard to know just where the mainstream is, but it's easy to recognize
constructions that are well outside it. Brand new features such as
\verb'//' comments and \verb'complex' in C, or features specific to one
architecture such as the keywords \verb'near' and \verb'far', are
guaranteed to cause trouble. If a feature is so unusual or unclear that to
understand it you need to consult a "language lawyer" -- an expert in
reading language definitions -- don't use it.

In this discussion, we'll focus on C and C++, general-purpose languages
commonly used to write portable software. The C standard is more than a
decade old and the language is very stable, but a new standard is in the
works, so upheaval (变动) is coming.  Meanwhile, the C++ standard is hot
off the press (刚刚出炉), so not all implementations have had time to
converge (聚合).

What is the C mainstream? The term usually refers to the established style
of use of the language, but sometimes it's better to plan for the future.
For example, the original version of C did not require function prototypes.
One declared \verb'sqrt' to be a function by saying
\begin{badcode}
    double sqrt();
\end{badcode}
which defines the type of the return value but not of the parameters. ANSI
C added function prototypes, which specify everything:
\begin{wellcode}
    double sqrt(double);
\end{wellcode}
ANSI C compilers are required to accept the earlier syntax, but you should
nonetheless (仍然) write prototypes for all your functions. Doing so will
guarantee safer code -- function calls will be fully type-checked -- and if
interfaces change, the compiler will catch them. If your code calls
\begin{wellcode}
    func(7, PI);
\end{wellcode}
but func has no prototype, the compiler might not verify that \verb'func'
is being called correctly. If the library later changes so that \verb'func'
has three arguments, the need to repair the software might be missed
because the old-style syntax disables type checking of function arguments.

C++ is a larger language with a more recent standard, so its mainstream is
harder to identify. For example, although we expect the STL to become
mainstream, this will not happen immediately, and some current
implementations do not support it completely.

\emph{Beware of language trouble spots (故障点).} As we mentioned, standard
leave some things intentionally undefined or unspecified, usually to give
compiler writers more flexibility. The list of such behaviors is
discouragingly long.

\textbf{Size of data types.} The sizes of basic data types in C and C++ are
not defined; other than (除了) the basic rules that
\begin{wellcode}
    sizeof(char) <= sizeof(short) <= sizeof(int) <= sizeof(long)
    sizeof(float) <= sizeof(double)
\end{wellcode}
and that \verb'char' must have at least 8 bit, \verb'short' and \verb'int'
at least 16, and \verb'long' at least 32, there are no guaranteed
properties. It's not even required that a pointer value fit in an
\verb'int'.

It's easy enough to find out what the sizes are for a specific compiler:
\begin{wellcode}
    /* sizeof: display sizes of basic types */
    int main(void)
    {
        printf("char %d, short %d, int %d, long %d",
                sizeof(char), sizeof(short),
                sizeof(int), sizeof(long));
        printf(" float %d, double %d, void* %d\n",
                sizeof(float), sizeof(double), sizeof(void *));
        return 0;
    }
\end{wellcode}
The output is the same on most of the machines we use regularly:
\begin{wellcode}
    char 1, short 2, int 4, long 4, float 4, double 8, void* 4
\end{wellcode}
but other values are certainly possible. Some 64-bit machines produce this:
\begin{wellcode}
    char 1, short 2, int 4, long 8, float 4, double 8, void* 8
\end{wellcode}
and early PC compilers typically produced this:
\begin{wellcode}
    char 1, short 2, int 2, long 4, float 4, double 8, void* 2
\end{wellcode}
In the early days of PCs, the hardware supported several kinds of pointers.
Coping with this mess (混乱) caused the invention (发明) of pointer
modifiers like \verb'far' and \verb'near', neither of which is standard,
but whose reserved-word ghosts (灵魂) still haunt (出没于) current
compilers. If your compiler can change the sizes of basic types, or if you
have machines with different sizes, try to compile and test your program in
these different configurations.

The standard header file \verb'stddef.h' defines a number of types that can
help with portability. The most commonly-used of these is \verb'size_t',
which is the unsigned integer type returned by the \verb'sizeof' operator.
Values of this type are returned by functions like \verb'strlen' and used
as arguments by many functions, including \verb'malloc'.

Learning from some of these experiences, Java defines the sizes of all
basic data types: \verb'byte' is 8 bits, \verb'char' and \verb'short' are
16, \verb'int' is 32, and \verb'long' is 64.

We will ignore the rich set of potential issues related to floating-point
computation since that is a book-sized topic in itself. Fortunately, most
modern machines support the IEEE standard for floating-point hardware, and
thus the properties of floating-point arithmetic are reasonably well
defined.

\textbf{Order of evaluation.} In C and C++, the order of evaluation of
operands of expressions, side effects, and function arguments is not
defined. For example, in the assignment
\begin{badcode}
    n = (getchar() << 8) | getchar();
\end{badcode}
the second \verb'getchar' could be called first: the way the expression is
written is not necessarily the way it executes. In the statement
\begin{badcode}
    ptr[count] = name[++count];
\end{badcode}
\verb'count' might be incremented before or after it is used to index
\verb'ptr', and in
\begin{badcode}
    printf("%c %c\n", getchar(), getchar());
\end{badcode}
the first input character could be printed second instead of first. In
\begin{badcode}
    printf("%f %s\n", log(-1.23), strerror(errno));
\end{badcode}
the value of \verb'errno' may be evaluated before \verb'log' is called.

There are rules for when certain expressions are evaluated. By definition,
all side effects and function calls must be completed at each semicolon, or
when a function is called. The \verb'&&' and \verb'||' operators execute
left to right and only as far as necessary to determine their truth value
(including side effects). The condition in a \verb'?:' operator is
evaluated (including side effects) and then exactly one of the two
expressions that follow is evaluated.

Java has a stricter definition of order of evaluation. It requires that
expressions, including side effects, be evaluated left to right, though
(虽然) one authoritative (权威的) manual advises not writing code that
depends "crucially (重要的)" on this behavior. This is sound (合理的)
advice if there's any chance that Java code will be converted to C or C++,
which make no such promises (约定). Converting between languages is an
extreme but occasionally reasonable test of portability.

\textbf{Signedness of \texttt{char}.} In C and C++, it is not specified
whether the \verb'char' data type is signed or unsigned. This can lead to
trouble when combining \verb'char's and \verb'int's, such as in code that
calls the \verb'int'-valued routine \verb'getchar()'. If you say
\begin{wellcode}
    char    c;  /* should be int */
    c = getchar();
\end{wellcode}
the value of \verb'c' will be between 0 and 255 if \verb'char' is unsigned,
and between -128 and 127 if \verb'char' is signed, for the almost universal
configuration of 8-bit characters on a two's complement machine. This has
implications if the character is to be used as an array subscript or if it
is to be tested against \verb'EOF', which usually has value -1 in
\verb'stdio'.  For instance, we had developed this code in Section
\ref{sec:test_as_you_write_code} after fixing a few boundary conditions in
the original version. The comparison \verb's[i] == EOF' will always fail if
\verb'char' is unsigned:
\begin{badcode}
    int i;
    char s[MAX];

    for (i = 0; i < MAX - 1; i++)
        if ((s[i] = getchar()) == '\n' || s[i] == EOF)
            break;
    s[i] = '\0';
\end{badcode}
When \verb'getchar' returns \verb'EOF', the value \verb'255' (\verb'0xff',
the result of converting \verb'-1' to \verb'unsigned char') will be stored
in \verb's[i]', If \verb's[i]' is unsigned, this will remain 255 for the
comparison with \verb'EOF', which will fail.

Even if \verb'char' is signed, however, the code isn't correct. The
comparison will succeed at \verb'EOF', but a valid input byte of
\verb'0xff' will look just like \verb'EOF' ad terminate the loop
prematurely (过早). So regardless of the sign of \verb'char', you must
always store the return value of \verb'getchar' in an \verb'int' for
comparison with \verb'EOF'. Here is how to write the loop portably:
\begin{wellcode}
    int c, i;
    char s[MAX];

    for (i = 0; i < MAX - 1; i++) {
        if ((c = getchar()) == '\n' || c == EOF)
            break;
        s[i] = c;
    }
    s[i] = '\0';
\end{wellcode}

Java has no \verb'unsigned' qualifier; integral types are signed and the
(16-bit) \verb'char' type is not.

\textbf{Arithmetic or logical shift.} Right shifts of signed quantities
with \verb'>>' operator may be arithmetic (a copy of the sign bit is
propagated (可繁殖的) during the shift) or logical (zeros fill the vacated
(空出的) bits during the shift). Again, learning from the problems with C
and C++, Java reserves \verb'>>' for arithmetic right shift and provides a
separate operator \verb'>>>' for logical right shift.

\textbf{Byte order.} The byte order within \verb'short', \verb'int', and
\verb'long' is not defined; the byte with the lowest address may be the
most significant byte or the least significant byte. This is a
hardware-dependent issue that we'll discuss later in this chapter.

\textbf{Alignment of structure and class members.} The alignment of items
within structures, classes, and unions is not defined, except that members
are laid out in the order of declaration. For example, in this structure
\begin{wellcode}
    struct X {
        char    c;
        int     i;
    };
\end{wellcode}
the address of \verb'i' could be 2,4, or 8 bytes from the beginning of the
structure. A few machines allow \verb'int's to be stored on odd boundaries,
but most demand that an n-byte primitive data type be stored at an n-byte
boundary, for example that \verb'doubles', which are usually 8 bytes long,
are stored at addresses that are multiples of 8.  On top of this, the
compiler writer may make further (更多的) adjustments, such as forcing
alignment for performance reasons.

You should never assume that the elements of a structure occupy contiguous
memory. Alignment restrictions introduce "holes"; \verb'struct X' will have
at least one byte of unused space. These holes imply that a structure may
be bigger than the sum of its member sizes, and will vary from machine to
machine. If you're allocating memory to hold one, you must ask for
\texttt{sizeof (struct X)} bytes, not \verb'sizeof(char) + sizeof(int)'.

\textbf{Bitfields.} Bitfields are so machine-dependent that no one should
use them.

This long list of perils (危险) can be skirted (绕过) by following a few
rules. Don't use side effects except for a very few idiomatic constructions
like
\begin{wellcode}
    a[i++] = 0;
    c = *p++;
    *s++ = *t++;
\end{wellcode}
Don't compare a \verb'char' to \verb'EOF'. Always use \verb'sizeof' to
compute the size of types and objects. Never right shift a signed value.
Make sure the data type is big enough for the range of values you are
storing in it.

\emph{Try several compilers.} It's easy to think that you understand
portability, but compilers will see problems that you don't, and different
compilers sometimes see your program differently, so you should take
advantage of their help. Turn on all compiler warnings. Try multiple
compilers on the same machine and on different machines.  Try a C++
compiler on a C program.

Since the language accepted by different compilers varies, the fact that
your program compiles with one compiler is no guarantee that it is even
syntactically correct.  If several compilers accept your code, however, the
odds (奇怪的事情) improve. We have compiled every C program in this book
with three C compilers on three unrelated operating systems (Unix, Plan 9,
Windows) and also a couple of C++ compilers. This was a sobering (合理的)
experience, but it caught dozens of portability errors that no amount of
human scrutiny (检查) would have uncovered. They were all trivial (琐细的)
to fix.

Of course, compilers cause portability problems too, by making different
choices for unspecified behaviors. But our approach still gives us hope.
Rather than writing code in a way that amplifies (放大) the differences
among systems, environments, and compilers, we strive (争取) to create
software that behaves independently of the variations. In short, we steer
clear of (避开) features and properties that are likely to vary.

\section{Headers and Libraries}
\label{sec:header_library}

Headers and libraries provide services that augment (增加) the basic
language.  Examples include input and output through \verb'stdio' in C,
\verb'iostream' in C++, and \verb'java.io' in Java.  Strictly speaking,
these are not part of the language, but they are defined along with the
language itself and are expected to be part of any environment that claims
to support it. But because libraries cover a broad spectrum (范围) of
activities, and must often deal with operating system issues, they can
still harbor (港湾) non-portabilities.

\emph{Use standard libraries.} The same general advice applies here as for
the core language: stick to the standard, and within its older,
well-established components. C defines a standard library of functions for
input and output, string operations, character class tests, storage
allocation, and a variety of other tasks. If you confine (禁止) your
operating system interactions to these functions, there is a good chance
that your code will behave the same way and perform well as it moves from
system to system. But you must still be careful, because there are many
implementations of the library and some of them contain features that are
not defined in the standard.

ANSI C does not define the string-copying function \verb'strdup', yet most
environments provide it, even those that claim to conform (一致) to the
standard. A seasoned (老练的) programmer may use \verb'strdup' out of habit
(出于习惯), and not be warned that it is non-standard.  Later, the program
will fail to compile when ported to an environment that does not provide
the function. This sort of problem is the major portability headache
introduced by libraries; the only solution is to stick to the standard and
test your program in a wide variety of environments.

Header files and package definitions declare the interface to standard
functions.  One problem is that headers tend to be cluttered (杂乱的)
because they are trying to cope (应付) with several languages in the same
file. For example, it is common to find a single header file like
\verb'stdio.h' serving pre-ANSI C, ANSI C, and even C++ compilers. In such
cases, the file is littered (弄乱) with conditional compilation directives
like \verb'#if' and \verb'#ifdef'.  Because the preprocessor language is
not very flexible, the files are complicated and hard to read, and
sometimes contain errors.

This excerpt (摘录) from a header file on one of our systems is better than
most, because it is neatly formatted:
\begin{wellcode}
    #ifdef __OLD_C
        extern int fread();
        extern int fwrite();
    #else
        #if defined(__STDC__) || defined(__cplusplus)
            extern size_t fread(void *, size_t, size_t, FILE *);
            extern size_t fwrite(const void *, size_t, size_t, FILE *);
        #else /* not __STDC__ __cplusplus */
            extern size_t fread();
            extern size_t fwrite();
        #endif /* else not __STDC__ || __cplusplus */
    #endif
\end{wellcode}
Even though the example is relatively clean, it demonstrates that header
files (and programs) structured like this are intricate (复杂的) and hard
to maintain. It might be easier to use a different header for each compiler
or environment. This would require maintaining separate files, but each
would be self-contained and appropriate for a particular system, and would
reduce the likelihood of errors like including \verb'strdup' in a strict
ANSI C environment.

Header files also can "pollute" the name space by declaring a function with
the same name as one in your program. For example, our warning-message
function \verb'weprintf' was originally called \verb'wprintf', but we
discovered that some environments, in anticipation (预测) of the new C
standard, define a function with that name in \verb'stdio.h'.  We needed to
change the name of our function in order to compile on those systems and be
ready for the future. If the problem was an erroneous implementation rather
than a legitimate (合法的) change of specification, we could work around it
by redefining the name when including the header:
\begin{badcode}
    /* some versions of stdio use wprintf so define it away */
    #define wprintf stdio_wprintf
    #undef wprintf
    /* code using our wprintf() follows ... */
\end{badcode}
This maps all occurrences of \verb'wprintf' in the header file to
\verb'stdio_wprintf' so they will not interfere with our version. We can
then use our own \verb'wprintf' without changing its name, at the cost of
some clumsiness (笨拙) and the risk that a library we link with will call
our \verb'wprintf' expecting to get the official one. For a single
function, it's probably not worth the trouble, but some systems make such a
mess of the environment that one must resort to (采取) extremes (极端方法)
to keep the code clean. Be sure to comment what the construction is doing,
and don't make it worse by adding conditional compilation. If some
environments define \verb'wprintf', assume they all do; then the fix is
permanent and you won't have to maintain the \verb'#ifdef' statements as
well.  It may be easier to switch than fight and it's certainly safer, so
that's what we did when we changed the name to \verb'weprintf'.

Even if you try to stick to the rules and the environment is clean, it is
easy to step outside the limits by implicitly assuming that some favorite
property is true everywhere. For instance, ANSI C defines six signals that
can be caught with \verb'signal'; the POSIX standard defines 19; most Unix
systems support 32 or more. If you want to use a non-ANSI signal, there is
clearly a tradeoff (交易) between functionality and portability, and you
must decide which matters more.

There are many other standards that are not part of a programming language
definition; examples include operating system and network interfaces,
graphics interfaces, and the like. Some are meant to carry across more than
one system, like POSIX; others are specific to one system, like the various
Microsoft Windows APIs.  Similar advice holds here as well. Your programs
will be more portable if you choose widely used and well-established
standards, and if you stick to the most central and commonly used aspects.

\section{Program Organization}
\label{sec:program_organization}

There are two major approaches to portability, which we will call union and
intersection. The union approach is to use the best features of each
particular system, and make the compilation and installation process
conditional on properties of the local environment. The resulting code
handles the union of all scenarios (方案), taking advantage of the
strengths of each system. The drawbacks include the size and complexity of
the installation process and the complexity of code riddled (充塞) with
compile-time conditionals.

\emph{Use only features available everywhere.} The approach we recommend is
intersection: use only those features that exist in all target systems;
don't use a feature if it isn't available everywhere. One danger is that
the requirement of universal availability of features may limit the range
of target systems or the capabilities of the program; another is that
performance may suffer in some environments.

To compare these approaches, let's look at a couple of examples that use
union code and rethink them using intersection. As you will see, union code
is by design unportable, despite (不管) its stated goal, while intersection
code is not only portable but usually simpler.

This small example attempts to cope with (应付) an environment that for
some reason doesn't have the standard header file \verb'stdlib.h':

\begin{wellcode}
    #if defined(STDC_HEADERS) || defined(_LIBC)
        #include <stdlib.h>
    #else
        extern void *malloc(unsigned int);
        extern void *realloc(void *, unsigned int);
    #endif
\end{wellcode}
This style of defense (防御) is acceptable if used occasionally, but not if
it appears often. It also begs the question of how many other functions
from \verb'stdlib' will eventually find their way into this or similar
conditional code. If one is using \verb'malloc' and \verb'realloc', surely
\verb'free' will be needed as well, for instance. What if
\verb'unsigned int' is not the same as \verb'size_t', the proper type of
the argument to
\verb'malloc' and \verb'realloc'? Moreover, how do we know that
\verb'STDC_HEADERS' or \verb'_LIBC' are defined, and defined correctly?
How can we be sure that there is no other name that should trigger (触发)
the substitution in some environment? Any conditional code like this is
incomplete-unportable -- because eventually a system that doesn't match the
condition will come along (出现), and we must edit the \verb'#ifdef's. If
we could solve the problem without conditional compilation, we would
eliminate the ongoing (进行中的) maintenance headache.

Still, the problem this example is solving is real, so how can we solve it
once and for all? Our preference would be to assume that the standard
headers exist; it's someone else's problem if they don't. Failing that, it
would be simpler to ship with (装载) the software a header file that
defines \verb'malloc', \verb'realloc', and \verb'free', exactly as ANSI C
defines them. This file can always be included, instead of applying
band-aids (创可帖) throughout the code. Then we will always know that the
necessary interface is available.

\emph{Avoid conditional compilation.} Conditional compilation with
\verb'#ifdef' and similar preprocessor directives is hard to manage,
because information tends to get sprinkled (点缀) throughout the source.
\begin{badcode}
    #ifdef NATIVE
        char *astring = "convert ASCII to native character set";
    #else
    #ifdef MAC
        char *astring = "convert to Mac text file format";
    #else
    #ifdef DOS
        char *astring = "convert to DOS text file format";
    #else
        char *astring = "convert to Unix text file format";
    #endif /* DOS */
    #endif /* MAC */
    #endif /* NATIVE */
\end{badcode}
This excerpt (摘录) would have been better with \verb'#elif' after each definition,
rather than having \verb'#endif's pile up (堆积) at the end. But the real problem is that, despite its
intention, this
code is highly non-portable because it behaves differently on each system
and needs
to be updated with a new \verb'#ifdef' for every new environment. A single string
with
more general wording would be simpler, completely portable, and just as
informative (提供消息的):
\begin{wellcode}
    char *astring = "convert to local text format";
\end{wellcode}
This needs no conditional code since it is the same on all systems.

Mixing compile-time control flow (determined by \verb'#ifdef' statements)
with run-time control flow is much worse, since it is very difficult to
read.
\begin{badcode}
    #ifndef DISKSYS
        for (i = 1; i <= msg->dbgmsg.msg_total; i++)
    #endif
    #ifdef DISKSYS
        i = dbgmsgno;
        if (i <= msg->dbgmsg.msg_total)
    #endif
        {
            ...
            if (msg->dbgmsg.msg_total == i)
    #ifndef DISKSYS
                break; /* no more messages to wait for */
            /*
             * about 30 more lines, with further Conditional
             * compilation.
             */
    #endif
        }
\end{badcode}

Even when apparently innocuous (无害的), conditional compilation can
frequently be replaced be cleaner methods. For instance, \verb'#ifdef's are
often used to control debugging code:
\begin{badcode}
    #ifdef DEBUG
        printf(...);
    #endif
\end{badcode}
but a regular \verb'if' statement with a constant condition may work just
as well:
\begin{wellcode}
    enum { DEBUG = 0 };
    ...
    if (DEBUG) {
        printf(...);
    }
\end{wellcode}
If \verb'DEBUG' is zero, most compilers won't generate any code for this,
but they will check the syntax of the excluded (排除的) code. An
\verb'#ifdef', by contrast, can conceal (掩盖) syntax errors that will
prevent compilation if the \verb'#ifdef' is later enabled.

Sometimes conditional compilation excludes large blocks of code:
\begin{wellcode}
    #ifdef notdef   /* undefined symbol */
        ...
    #endif

    #if 0
        ...
    #endif
\end{wellcode}
but conditional code can often be avoided altogether by using files that
are conditionally substituted during compilation. We will return to this
topic in the next section.

When you must modify a program to adapt to a new environment, don't begin
by making a copy of the entire program. Instead, adapt the existing source.
You will probably need to make changes to the main body of the code, and if
you edit a copy, before long (不久以后) you will have divergent (分散的)
versions. As much as possible, there should only be a single source for a
program; if you find you need to change something to port to a particular
environment, find a way to make the change work everywhere.  Change
internal interfaces if you need to, but keep the code consistent and
\verb'#ifdef'-free. This will make your code more portable over time,
rather than more specialized.  Narrow the intersection, don't broaden the
union.

We have spoken out against conditional compilation and shown some of the
problems it causes. But the nastiest (污秽的) problem is one we haven't
mentioned: it is almost impossible to test. An \verb'#ifdef' turns a single
program into two separately-compiled programs. It is difficult to know
whether all the variant programs have been compiled and tested. If a change
is made in one \verb'#ifdef' block, we may need to make it in others, but
the changes can be verified only within the environment that causes those
\verb'#ifdef's to be enabled. If a similar change needs to be made for
other configurations, it cannot be tested. Also, when we add a new
\verb'#ifdef' block, it is hard to isolate the change to determine what
other conditions need to be satisfied to get here, and where else this
problem might need to be fixed. Finally, if something is in code that is
conditionally omitted, the compiler doesn't see it. It could be utter
(完全) nonsense (说不通) and we won't know until some unlucky customer
tries to compile it in the environment that triggers that condition. This
program compiles when \verb'_MAC' is defined and fails when it is not:
\begin{wellcode}
    #ifdef _MAC
        printf("This is Macintosh\r");
    #else
        This will give a syntax error on other systems
    #endif
\end{wellcode}

So our preference is to use only features that are common to all target
environments. We can compile and test all the code. If something is a
portability problem, we rewrite to avoid it rather than adding conditional
code; this way, portability will steadily increase and the program itself
will improve rather than becoming more complicated.

Some large systems are distributed with a configuration script to tailor
(裁减) code to the local environment. At compilation time, the script tests
the environment properties -- location of header files and libraries, byte
order within words, size of types, implementations known to be broken
(surprisingly common), and so on -- and generates configuration parameters
or makefiles that will give the right configuration settings for that
situation, These scripts can be large and intricate (复杂的), a significant
fraction (成分) of a software distribution, and require continual (持续的)
maintenance to keep them working. Sometimes such techniques are necessary
but the more portable and \verb'#ifdef'-free the code is, the simpler and
more reliable the configuration and installation will be.

\begin{exercise}
    Investigate how your compiler handles code contained within a
    conditional block like
    \begin{wellcode}
        const int DEBUG = 0;
        /* or enum { DEBUG = 0 }; */
        /* or final boolean DEBUG = false; */

        if (DEBUG) {
            ...
        }
    \end{wellcode}
    Under what circumstances does it check syntax? When does it generate
    code? If you have access to more than one compiler, how do the results
    compare?
\end{exercise}

\section{Isolation}
\label{sec:isolation}

Although we would like to have a single source that compiles without change
on all systems, that may be unrealistic. But it is a mistake to have
non-portable code scattered (分散) throughout a program: that is one of the
problems that conditional compilation creates.

\emph{Localize system dependencies in separate files.} When different code
is needed for different systems, the differences should be localized in
separate files, one file for each system. For example, the text editor Sam
runs on Unix, Windows, and several other operating systems. The system
interfaces for these environments vary widely, but most of the code for Sam
is identical everywhere. A single file captures the system variations for a
particular environment; \verb'unix.c' provides the interface code for Unix
systems, and \verb'windows.c' for the Windows environment. These files
implement a portable interface to the operating system and hide the
differences. Sam is, in effect, written to its own virtual operating
system, which is ported to various real systems by writing a couple of
hundred lines of C to implement half a dozen small but non-portable
operations using locally available system calls.

The graphics environments of these operating systems are almost unrelated.
Sam copes (应付) by having a portable library for its graphics. Although
it's a lot more work to build such a library than to hack (劈砍) the code
to adapt to a given system -- the code to interface to the X Window system,
for example, is about half as big as the rest of Sam put together -- the
cumulative (累积的) effort is less in the long run. And as a side benefit,
the graphics library is itself valuable, and has been used separately to
make a number of other programs portable, too.

Sam is an old program; today, portable graphics environments such as
OpenGL, Tcl/Tk and Java are available for a variety of platforms. Writing
your code with these rather than a proprietary (私有的) graphics library
will give your programs wider utility.

\emph{Hide system dependencies behind interfaces.} Abstraction is a
powerful technique for creating boundaries between portable and
non-portable parts of a program.  The I/O libraries that accompany (伴随)
most programming languages provide a good example: they present an
abstraction of secondary storage in terms of (在...方面) files to be opened
and closed, read and written, without any reference to their physical
location or structure. Programs that adhere (依附) to the interface will
run on any system that implements it.

The implementation of Sam provides another example of abstraction. An
interface is defined for the file system and graphics operations and the
program uses only features of the interface. The interface itself uses
whatever facilities are available in the underlying system. That might
require significantly different implementations on different systems, but
the program that uses the interface is independent of that and should
require no changes as it is moved.

The Java approach to portability is a good example of how far this can be
carried.  A Java program is translated into operations in a "virtual
machine." that is, a simulated computer that can be implemented to run on
any real machine. Java libraries provide uniform access to features of the
underlying system, including graphics, user interface, networking, and the
like; the libraries map into whatever the local system provides. In theory,
it should be possible to run the same Java program (even after translation)
everywhere without change.
