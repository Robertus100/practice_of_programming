% vim: ts=4 sts=4 sw=4 et tw=75
\chapter{Design and Implementation}
\label{chap:desipl}
\begin{quote}
    Show me your flowcharts and conceal your tables, and I shall continue
    to be mystified. Show me your tables, and I won't usually need your
    flowcharts; they'll be obvious.
\end{quote}

\begin{quotesrc}
    Frederick P.Brooks, Jr.,\bookname{The Mythical Man Month}
\end{quotesrc}

As the quotation from Brooks's classic book suggests, the design of the
data structures is the central decision in the creation of a program. Once
the data structures are laid out, the algorithms tend to fall into place,
and the coding is comparatively easy.

This point of view is oversimplified but not misleading. In the previous
chapter we examined the basic data structures that are the building blocks
of most programs. In this chapter we will combine such structures as we
work through the design and implementation of a modest-sized program. We
will show how the problem influences the data structures, and how the code
that follows is straightforward once we have the data structures mapped out.

One aspect of this point of view is that the choice of programming Language
is relatively unimportant to the overall design. We will design the
program in the abstract and then write it in C, Java, C++, Awk, and Perl.
Comparing the implementations demonstrates how languages can help or
hinder, and ways in which they are unimportant. Program design can
certainly be colored by a language but is not usually dominated by it.

The problem we have chosen is unusual, but in basic from it is typical of
many programs: some data comes in, some data goes out, and the processing
depends on a little ingenuity.

Specifically, we're going to generate random English text that reads well.
If we emit random letters or random words, the result will be nonsense. For
example, a program that randomly selects letters(and blanks. to separate
words)might produce this:
\begin{wellcode}
    xptmxgn xusaja afqnzgxl lhidwed rjdjuvpydrlwnjy
\end{wellcode}
which is not very convincing. If we weight the letters by their frequency
of appearance in English text, we might get this:
\begin{wellcode}
    idtefoae tcs trder jcii ofdslnqetacp t ola
\end{wellcode}
which isn't a great deal better. Words chosen from the dictionary at random
don't make much more sense:
\begin{wellcode}
    polydactyl equatorial splashily jowl verandah circumscribe
\end{wellcode}
For better results, we need a statistical model with more structure.Such as
the frequency of appearance of whole phrases. But where can we find such
statistics?

We could grab a large body of English and study it in detail, but there is
an easier and more entertaining approach. The key observation is that we
can use any existing text to construct a statistical model of the language
\textit{as used} in \textit{that text}, and from that generate random text
that has similar statistics to the original.

\section{The Markov Chain Algorithm}
\label{sec:the_markov_chain_algorithm}

An elegant way to do this sort of processing is a technique called a
\textit{Markov chain algorithm}. If we imagine the input as a sequence of
overlapping phrases, the algorithm divides each phrase into two parts, a
multi-word \textit{prefix} and a single \textit{suffix} word that follows
the prefix. A Markov chain algorithm emits output phrases by randomly
choosing the suffix that follows the prefix, according to the statistics
of(in our case)the original text. Three-word phrases work well a two-word
prefix is used to select the suffix word:
\\
\indent set \textit{$w_1$} and \textit{$w_2$} to the first two words in the text 
\\
\indent print \textit{$w_1$} and \textit{$w_2$} 
\\
\indent loop:
\\
\indent\indent randomly choose \textit{$w_3$}, one of the successors of
    prefix \textit{$w_1$} \textit{$w_2$} in the text
\\
\indent\indent print \textit{$w_{3}$}
\\
\indent\indent replace \textit{w} and \textit{$w_2$} by \textit{$w_2$} and
        \textit{$w_3$}
\\
\indent\indent repeat loop
\\
To illustrate, suppose we want to generate random text base on a few
sentences paraphrased from the epigraph above, using-word prefixes:
\\
\indent Show your flowcharts and conceal your tables and I will be
mystified. Show your tables and you flowcharts will be obvious.\textit{(end)}
\\
These are some of the pairs of input words and the words that follow them:
\\
\indent \begin{tabular}{ll}
    \textit{Input prefix:} & \textit{Suffix words that follow:}\\
    Show your & flowcharts tables\\
    your flowcharts & and will\\
    flowcharts and & conceal\\
    flowcharts will & be\\
    your tables & and and\\
    will be & mystified. obvious.\\
    be mystified. & Show\\
    be obvious. & \textit{(end)}
\indent \end{tabular} \\
A Markov algorithm processing this text will begin by printing \textit{Show
    your} and will then randomly pick either \textit{flowcharts} or
\textit{tables}. If it choose the former, the current prefix becomes
\textit{your flowcharts} and next word will be \textit{and} or
\textit{will}. If it chooses \textit{tables}, the next word will be
\textit{and}. This continues until enough output has been generated or
until the end-marker is encountered as a suffix.

Our program will read a piece of English text and use a Markov chain
algorithm to generate new text based on the frequency of appearance of
phrases of a fixed length. The number of words in the prefix, which is two
in our example, is a parameter. Making the prefix shorter tends to produce
less coherent prose; making it longer tends to reproduce the input text
verbatim. For English text, using two words to select a third is a good
compromise; it seems to recreate the flavor of the input while adding its
own whimsical touch.

What is a word? The obvious answer is a sequence of alphabetic characters,
but it is desirable to leave punctuation attached to the words so
\textit{"words"} and \textit{"words."} are different. This helps to improve
the quality of the generated prose by letting punctuation, and
therefore(indirectly)grammar, influence the word choice, although it also
permits unbalanced quotes and parentheses to sneak in. We will therefore
define a "word" as anything between white space, a decision that places no
\textit{restriction} on input language and leaves punctuation attached to
the words. Since most programming languages have facilities to split text
into white-space-separated words, this is also easy to implement.

Because of the method, all words, all two-word phrases, and all three-word
phrases in the output must have appeared in the input, but there should be
many four-word and longer phrases that are synthesized. Here are a few
sentences produced by the program we will develop in this chapter, when
given the text of Chapter VII of
\bookname{The Sun Also Rises} by Ernest Hemingway:
\begin{oldquote}
    As I started up the undershirt onto his chest black, and big stomach
    muscles bulging under the light. "You see them?" Below the line where
    his ribs stopped were two raised white welts. "See on the forehead."
    "Oh, Brett, I love you." "Let's not talk. Talking's all bilge. I'm
    going away tomorrow." "Tomorrow?" "Yes. Didn't I say so? I am." "Let's
    have a drink, then."
\end{oldquote}

We were lucky here that punctuation (标点符号) came out correctly; that
need not happen.

\section{Data Structure Alternatives}
\label{sec:data_structure_alternatives}

How much input do we intend to deal with? How fast must the program run? It
seems reasonable to ask our program to read in a whole book, so we should
be prepared for input sizes of $n = 100,000$ words or more. The output will
be hundreds or perhaps thousands of words, and the program should run in a
few seconds instead of minutes. With 100,000 words of input text, $n$ is
fairly large so the algorithms can't be too simplistic if we want the
program to be fast.

The Markov algorithm must see all the input before it can begin to generate
output, so it must store the entire input in some form. One possibility is
to read the whole input and store it in a long string, but we clearly want
the input broken down into words. If we store it as an array of pointers to
words, output generation is simple: to produce each word, scan the input
text to see what possible suffix words follow the prefix that was just
emitted, and then choose one at random. However, that means scanning all
100,000 input words for each word we generate; 1,000 words of output means
hundreds of millions of string comparisons, which will not be fast.

Another possibility is to store only unique input words, together with a
list of where they appear in the input so that we can locate successor
words more quickly.  We could use a hash table like the one in Chapter
\ref{chap:alds}, but that version doesn't directly address the needs of the
Markov algorithm, which must quickly locate all the suffixes of a given
prefix.

We need a data structure that better represents a prefix and its associated
suffixes.  The program will have two passes, an input pass that builds the
data structure representing the phrases, and an output pass that uses the
data structure to generate the random output. In both passes, we need to
look up a prefix (quickly): in the input pass to update its suffixes, and
in the output pass to select at random from the possible suffixes. This
suggests a hash table whose keys are prefixes and whose values are the sets
of suffixes for the corresponding prefixes.

For purposes of description, we'll assume a two-word prefix, so each output
word is based on the pair of words that precede it. The number of words in
the prefix doesn't affect the design and the programs should handle any
prefix length, but selecting a number makes the discussion concrete. The
prefix and the set of all its possible suffixes we'll call a state, which
is standard terminology for Markov algorithms.

Given a prefix, we need to store all the suffixes that follow it so we can
access them later. The suffixes are unordered and added one at a time. We
don't know how many there will be, so we need a data structure that grows
easily and efficiently, such as a list or a dynamic array. When we are
generating output, we need to be able to choose one suffix at random from
the set of suffixes associated with a particular prefix. Items are never
deleted.

What happens if a phrase appears more than once? For example, 'might appear
twice' might appear twice but 'might appear once' only once. This could be
represented by putting 'twice' twice in the suffix list for 'might appear'
or by putting it in once, with an associated counter set to 2. We've tried
it with and without counters; without is easier, since adding a suffix
doesn't require checking whether it's there already, and experiments showed
that the difference in run-time was negligible (可以忽略的).

In summary, each state comprises (包含) a prefix and a list of suffixes.
This information is stored in a hash table, with prefix as key. Each prefix
is a fixed-size set of words.  If a suffix occurs more than once for a
given prefix, each occurrence will be included separately (单独地) in the
list.

The next decision is how to represent the words themselves. The easy way is
to store them as individual strings. Since most text has many words
appearing multiple times, it would probably save storage if we kept a
second hash table of single words, so the text of each word was stored only
once. This would also speed up hashing of prefixes, since we could compare
pointers rather than individual characters: unique strings have unique
addresses. We'll leave that design as an exercise; for now, strings will be
stored individually.

\section{Building the Data structure in C}
\label{sec:building_the_data_structure_in_c}

Let's begin with a C implementation. The first step is to define some
constants.
\begin{wellcode}
    enum {
        NPREF   = 2,        /* number of prefix words */
        NHASH   = 4093,     /* size of state hash table array */
        MAXGEN  = 10000     /* maximum words generated */
    };
\end{wellcode}
This declaration defines the number of words (\verb'NPREF') for the prefix,
the size of the hash table array (\verb'NHASH'), and an upper limit on the
number of words to generate (\verb'MAXGEN'). If \verb'NPREF' is a
compile-time constant rather than a run-time variable, storage management
is simpler. The array size is set fairly large because we expect to give
the program large input documents, perhaps a whole book. We chose
\verb'NHASH = 4093' so that if the input has 10,000 distinct prefixes (word
pairs), the average chain will be very short, two or three prefixes. The
larger the size, the shorter the expected length of the chains and thus the
faster the lookup. This program is really a toy, so the performance isn't
critical, but if we make the array too small the program will not handle
our expected input in reasonable time; on the other hand, if we make it too
big it might not fit in the available memory.

The prefix can be stored as an array of words. The elements of the hash
table will be represented as a \verb'State' data type, associating the
\verb'Suffix' list with the prefix:
\begin{wellcode}
    typedef struct State State;
    typedef struct Suffix Suffix;
    struct State {  /* prefix + suffix list */
        char    *pref[NPREF];   /* prefix words */
        Suffix  *suf;           /* list of suffixes */
        State   *next;          /* next in hash table */
    };

    struct Suffix { /* list of suffixes */
        char    *word;  /* suffix */
        Suffix  *next;  /* next in list of suffixes */
    };

    State   *statetab[NHASH];   /* hash table of states */
\end{wellcode}
Pictorially (形象地), the data structures look like this:
%TODO: lack of a image

We need a hash function for prefixes, which are arrays of strings. It is
simple to modify the string hash function \verb'fmm' Chapter
\ref{chap:alds} to loop over the strings in the array, thus in effect
hashing the concatenation of the strings:
\begin{wellcode}
    /* hash: compute hash value for array of NPREF strings */
    unsigned int hash(char *s[NPREF])
    {
        unsigned int    h;
        unsigned char   *p;
        int i;

        h = 0;
        for (i = 0; i < NPREF; i++)
            for (p = (unsigned char *)s[i]; *p != '\0'; p++)
                h = MULTIPLIER * h + *p;
        return h % NHASH;
    }
\end{wellcode}

A similar modification to the \verb'lookup' routine completes the
implementation of the hash table:
\begin{wellcode}
    /* lookup: search for prefix; create if requested. */
    /* return pointer if present or created; NULL if not */
    /* creation doesn't strdup so strings mustn't change later */
    State *lookup(char *prefix[NPREF], int create)
    {
        int i, h;
        State *sp;

        h = hash(prefix);
        for (sp = statetab[h]; sp != NULL; sp = sp->next) {
            for (i = 0; i < NPREF; i++)
                if (strcmp(prefix[i], sp->pref[i]) != 0)
                    break;
            for (i == NPREF)    /* found it */
                return sp;
        }
        if (create) {
            sp = (State *)emalloc(sizeof(State));
            for (i = 0; i < NPREF; i++)
                sp->pref[i] = prefix[i];
            sp->suf = NULL;
            sp->next = statetab[h];
            statetab[h] = sp;
        }
        return sp;
    }
\end{wellcode}

Next we need to build the hash tables as the file is read:
\begin{wellcode}
    /* build: read input, build prefix table */
    void build(char *prefix[NPREF], FILE *f)
    {
        char    buf[100], fmt[10];

        /* create a format string; %s could overflow buf */
        sprintf(fmt, "%%%ds", sizeof(buf) - 1);
        while (fscanf(f, fmt, buf) != EOF)
            add(prefix, estrdup(buf));
    }
\end{wellcode}

The peculiar (罕见的) call to \verb'sprintf' gets around an irritating
(气人的) problem with \verb'fscanf', which is otherwise perfect for the
job. A call to \verb'fscanf' with format \verb'%s' will read the next
white-space-delimited word from the file into the buffer, but there is no
limit on size: a long word might overflow the input buffer, wreaking havoc
(报复性地破坏). If the buffer is 100 bytes long (which is far beyond what
we expect ever to appear in normal text), we can use the format \verb'%99s'
(leaving one byte for the terminal \verb"'\0'"), which tells \verb'fscanf'
to stop after 99 bytes. A long word will be broken into pieces, which is
unfortunate but safe. We could declare
\begin{badcode}
    enum { BUFSIZE = 100 };
    char    fmt[] = "%99s"; /* BUFSIZE - 1 */
\end{badcode}
but that requires two constants for one arbitrary decision -- the size of
the buffer -- and introduces the need to maintain their relationship. The
problem can be solved once and for all by creating the format string
dynamically with \verb'sprintf', so that's the approach we take.

The two arguments to build are the prefix array holding the previous
\verb'NPREF' words of input and a \verb'FILE' pointer. It passes the prefix
and a copy of the input word to add, which adds the new entry to the hash
table and advances the prefix:
\begin{wellcode}
    /* add: add word to suffix list, update the prefix */
    void add(char *prefix[NPREF], char *suffix)
    {
        State   *sp;

        sp = lookup(prefix, 1); /* create if not found */
        addsufix(sp, suffix);
        /* move the words down the prefix */
        memmove(prefix, prefix+1, (NPREF-1)*sizeof(prefix[0]));
        prefix[NPREF-1] = suffix;
    }
\end{wellcode}

The call to \verb'memmove' is the idiom for deleting from an array. It
shifts elements \verb'1' through \verb'NPREF-1' in the prefix down to
positions \verb'0' through \verb'NPREF-2', deleting the first prefix word
and opening a space for a new one at the end.

The \verb'addsuffix' routine adds the new suffix:
\begin{wellcode}
    /* addsuffix: add to state. suffix must not change later */
    void addsuffix(State *sp, char *suffix)
    {
        Suffix  *suf;

        suf = (Suffix *)emalloc(sizeof(Suffix));
        suf->word = suffix;
        suf->next = sp->suf;
        sp->suf = suf;
    }
\end{wellcode}
We split the action of updating the state into two functions: \verb'add'
performs the general service of adding a suffix to a prefix, while
\verb'addsuffix' performs the implementation-specific action of adding a
word to a suffix list. The \verb'add' routine is used by \verb'build', but
\verb'addsuffix' is used internally only by \verb'add'; it is an
implementation detail that might change and it seems better to have it in a
separate function, even though it is called in only one place.
